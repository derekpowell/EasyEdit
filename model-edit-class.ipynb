{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir('/scratch/dmpowell'):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = '/scratch/dmpowell/.cache/huggingface'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class for model editing and evaluation\n",
    "\n",
    "Need a wrapper class/function for edited models for generating/probing for evaluation. Ideally, evaluation is based on final token probability for each query. Probably top-k accuracy? (i.e. is targeted token in the top-k?) Or by post-edit rank? log rank? Or could be multiple choice? Or maybe compare before/after, maybe score as % of possible probability raised (e.g. from .2 to .8 = 75%)? Or just like, top-k accuracy? (i.e. is targeted token in the top-k?) Or by post-edit rank? log rank?\n",
    "\n",
    "- Takes model, tokenizer, modifications, etc.\n",
    "\t- For ICE can just prepend a prompt to \"imagine\"\n",
    "- Has following functions\n",
    "\t- for evaluation\n",
    "\t\t- `generate(prompt)` \n",
    "\t\t- `logits(prompt)` \n",
    "\t\t- `choose(prompt, options)` function for multiple choice\n",
    "\t\t- `top_k(prompt, k=5)` return top-k tokens\n",
    "\t\t- `in_top_k(prompt, token, k=5)` check if token in top-k tokens\n",
    "\t- `.init(model, edit_params)` will initialize model and save relevant weights\n",
    "\t- `.edit(request)` will do a requested edit\n",
    "\t- `.restore()` will restore original weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'experiments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/dmpowell/work/EasyEdit/model-edit-class.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bc001solrcasuedu/home/dmpowell/work/EasyEdit/model-edit-class.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bc001solrcasuedu/home/dmpowell/work/EasyEdit/model-edit-class.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcontextlib\u001b[39;00m \u001b[39mimport\u001b[39;00m redirect_stdout\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bc001solrcasuedu/home/dmpowell/work/EasyEdit/model-edit-class.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexperiments\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdemo\u001b[39;00m \u001b[39mimport\u001b[39;00m demo_model_editing, stop_execution, edit_model\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bc001solrcasuedu/home/dmpowell/work/EasyEdit/model-edit-class.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m nethook\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bc001solrcasuedu/home/dmpowell/work/EasyEdit/model-edit-class.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# from util.generate import generate_fast # adding\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'experiments'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, AutoModel, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from experiments.py.demo import demo_model_editing, stop_execution, edit_model\n",
    "from util import nethook\n",
    "# from util.generate import generate_fast # adding\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 619/619 [00:00<00:00, 188kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 1.61MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 8.42MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 5.57MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 4.04k/4.04k [00:00<00:00, 9.99MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 357/357 [00:00<00:00, 1.01MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 930/930 [00:00<00:00, 396kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 24.2G/24.2G [03:22<00:00, 119MB/s] \n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\" # gpt2-xl / \"EleutherAI/gpt-j-6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token(token):\n",
    "    token = \" \" + token if token[0] != \" \" else token\n",
    "    return(token)\n",
    "\n",
    "\n",
    "def encode_token(token:str, tokenizer):        \n",
    "    token = pad_token(token)\n",
    "    token_id = tokenizer(token)[\"input_ids\"]\n",
    "\n",
    "    return(token_id)\n",
    "\n",
    "\n",
    "class EditedModel:\n",
    "    def __init__(self, model, tok, hparams = None):\n",
    "        self.model = model\n",
    "        self.tok = tok\n",
    "        self.params = hparams\n",
    "        self.preprompt = \"\"\n",
    "        self.saved_weights = None\n",
    "        \n",
    "        self.tok.padding_side = \"left\"\n",
    "        self.tok.pad_token = self.tok.eos_token\n",
    "\n",
    "\n",
    "    def update_params(self, hparams):\n",
    "        self.params = hparams\n",
    "        self.preprompt = \"\"\n",
    "\n",
    "    \n",
    "    def edit(self, request):\n",
    "        \n",
    "        if self.params[\"mode\"] == \"ICE\":\n",
    "            self.preprompt = request[\"preprompt\"]\n",
    "\n",
    "        else:\n",
    "            with redirect_stdout(None):\n",
    "\n",
    "                self.model, self.saved_weights = edit_model(\n",
    "                    self.model, self.tok, [request], alg_name= self.params[\"mode\"]\n",
    "                )\n",
    "        \n",
    "    \n",
    "    def restore(self):\n",
    "\n",
    "        self.preprompt = \"\"\n",
    "        \n",
    "        if self.saved_weights:\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for k, v in self.saved_weights.items():\n",
    "                        nethook.get_parameter(self.model, k)[...] = v\n",
    "                self.saved_weights = None\n",
    "                # print(\"Original model restored\")\n",
    "            except NameError as e:\n",
    "                print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "            \n",
    "    def generate_text(self, texts, **kwargs):\n",
    "        \n",
    "        if type(texts) != list:\n",
    "            texts = [texts]\n",
    "        \n",
    "        texts = [self.preprompt + t for t in texts]\n",
    "\n",
    "        tokenizer = self.tok\n",
    "        encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(**encoding, **kwargs) # \n",
    "\n",
    "            generated_texts = tokenizer.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "        return(generated_texts)\n",
    "\n",
    "    \n",
    "    def token_logit(self, texts, token, start_ind = None):\n",
    "        \n",
    "        texts = self.preprompt + texts\n",
    "    \n",
    "        tokenizer = self.tok \n",
    "        model = self.model\n",
    "        encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_out = model(encoding[\"input_ids\"])\n",
    "            logits = model_out.logits\n",
    "            logprobs = F.log_softmax(logits, -1)\n",
    "\n",
    "        token_id = encode_token(token, tokenizer)\n",
    "        start_ind = -len(token_id)-1 if not start_ind else start_ind\n",
    "        \n",
    "        l = logprobs[:, start_ind:-1, token_id]\n",
    "        if len(l.squeeze().shape) == 0:\n",
    "            return(l.squeeze())\n",
    "        else:\n",
    "            return(l.squeeze().diag().sum())\n",
    "        \n",
    "\n",
    "    def choose(self, prompt, choices):\n",
    "        prompts = [prompt + pad_token(c) for c in choices]\n",
    "        logits = [self.token_logit(prompts[i], choices[i]) for i in range(len(choices))]\n",
    "        return(logits.index(max(logits)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = EditedModel(model, tokenizer)\n",
    "m = EditedModel(model, tokenizer, {\"mode\":\"ICE\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# m.edit({\"preprompt\": \"Imagine that a terrier is a kind of horse. In this case: \"})\n",
    "print(m.choose(\"A terrier is something people like to\", [\"pet\", \"eat\", \"ride\"]))\n",
    "m.restore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick testing with ROME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.9278, device='cuda:0')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite = {\n",
    "    'prompt': '{} plays',\n",
    "    'target_new': {'str': 'baseball'},\n",
    "    'target_true':{'str':'basketball'},\n",
    "    'subject': 'LeBron James'\n",
    "}\n",
    "\n",
    "m2 = EditedModel(model, tokenizer, {\"mode\":\"ROME\"})\n",
    "m2.token_logit(\"LeBron James plays baseball\", \"baseball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0070, device='cuda:0')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.edit(rewrite)\n",
    "m2.token_logit(\"LeBron James plays baseball\", \"baseball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.9278, device='cuda:0')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.restore()\n",
    "m2.token_logit(\"LeBron James plays baseball\", \"baseball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>property</th>\n",
       "      <th>query_fwd</th>\n",
       "      <th>query_rev</th>\n",
       "      <th>answer_fwd</th>\n",
       "      <th>answer_rev</th>\n",
       "      <th>foil1</th>\n",
       "      <th>foil2</th>\n",
       "      <th>foil3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bee</td>\n",
       "      <td>can_fly</td>\n",
       "      <td>&lt;subj&gt; can &lt;answer&gt;</td>\n",
       "      <td>one animal that can &lt;answer&gt; is a &lt;subj&gt;</td>\n",
       "      <td>fly</td>\n",
       "      <td>&lt;subj&gt;</td>\n",
       "      <td>run</td>\n",
       "      <td>gallop</td>\n",
       "      <td>swim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bee</td>\n",
       "      <td>makes_sound</td>\n",
       "      <td>a sound a &lt;subj&gt; makes is &lt;answer&gt;</td>\n",
       "      <td>&lt;answer&gt; is a sound made by a &lt;subj&gt;</td>\n",
       "      <td>buzz</td>\n",
       "      <td>&lt;subj&gt;</td>\n",
       "      <td>bark</td>\n",
       "      <td>moo</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bee</td>\n",
       "      <td>genus</td>\n",
       "      <td>a &lt;subj&gt; is a &lt;answer&gt;</td>\n",
       "      <td>one type of &lt;answer&gt; is a &lt;subj&gt;</td>\n",
       "      <td>insect</td>\n",
       "      <td>&lt;subj&gt;</td>\n",
       "      <td>mammal</td>\n",
       "      <td>reptile</td>\n",
       "      <td>aves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bee</td>\n",
       "      <td>has_wings</td>\n",
       "      <td>&lt;subj&gt; have &lt;answer&gt;</td>\n",
       "      <td>&lt;answer&gt; are found on &lt;subj&gt;</td>\n",
       "      <td>wings</td>\n",
       "      <td>&lt;subj&gt;</td>\n",
       "      <td>fins</td>\n",
       "      <td>four legs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>has_wings</td>\n",
       "      <td>&lt;subj&gt; have &lt;answer&gt;</td>\n",
       "      <td>&lt;answer&gt; are found on &lt;subj&gt;</td>\n",
       "      <td>wings</td>\n",
       "      <td>&lt;subj&gt;</td>\n",
       "      <td>fins</td>\n",
       "      <td>four legs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity     property                           query_fwd   \n",
       "0    bee      can_fly                 <subj> can <answer>  \\\n",
       "1    bee  makes_sound  a sound a <subj> makes is <answer>   \n",
       "2    bee        genus              a <subj> is a <answer>   \n",
       "3    bee    has_wings                <subj> have <answer>   \n",
       "4   bird    has_wings                <subj> have <answer>   \n",
       "\n",
       "                                  query_rev answer_fwd answer_rev   foil1   \n",
       "0  one animal that can <answer> is a <subj>        fly     <subj>     run  \\\n",
       "1      <answer> is a sound made by a <subj>       buzz     <subj>    bark   \n",
       "2          one type of <answer> is a <subj>     insect     <subj>  mammal   \n",
       "3              <answer> are found on <subj>      wings     <subj>    fins   \n",
       "4              <answer> are found on <subj>      wings     <subj>    fins   \n",
       "\n",
       "       foil2 foil3  \n",
       "0     gallop  swim  \n",
       "1        moo  meow  \n",
       "2    reptile  aves  \n",
       "3  four legs   NaN  \n",
       "4  four legs   NaN  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv(\"animal-data.tsv\", sep=\"\\t\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>orig_entity</th>\n",
       "      <th>variable</th>\n",
       "      <th>subj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>typical_token_y</td>\n",
       "      <td>Siamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>cow</td>\n",
       "      <td>typical_token_y</td>\n",
       "      <td>Holstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>pig</td>\n",
       "      <td>typical_token_y</td>\n",
       "      <td>Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>bird</td>\n",
       "      <td>typical_token_y</td>\n",
       "      <td>sparrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog</td>\n",
       "      <td>bee</td>\n",
       "      <td>typical_token_y</td>\n",
       "      <td>bumblebee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity orig_entity         variable       subj\n",
       "0    dog         cat  typical_token_y    Siamese\n",
       "1    dog         cow  typical_token_y   Holstein\n",
       "2    dog         pig  typical_token_y  Hampshire\n",
       "3    dog        bird  typical_token_y    sparrow\n",
       "4    dog         bee  typical_token_y  bumblebee"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_method = \"ICE\"\n",
    "# hparams = ...\n",
    "edited_model = EditedModel(model, tokenizer, {\"mode\": edit_method})\n",
    "\n",
    "\n",
    "types_df = pd.read_csv(\"animal-type-tokens.tsv\", sep=\"\\t\")\n",
    "eval_df = pd.read_csv(\"animal-data.tsv\", sep=\"\\t\")\n",
    "edits_df = (\n",
    "    pd.merge(types_df, types_df, how = \"cross\")\n",
    "    .loc[lambda x: x.entity_type_x!=x.entity_type_y] \n",
    "    .filter(['entity_type_x', 'entity_type_y', 'typical_token_y', 'rare_token_y'])\n",
    "    .assign(novel_token = \"dax\")\n",
    "    .rename(columns = {\"entity_type_y\": \"orig_entity\"})\n",
    "    .melt(['entity_type_x', \"orig_entity\"])\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={\"entity_type_x\":\"entity\", \"value\":\"subj\"})\n",
    ")\n",
    "edits_df.head()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "corr_answers = []\n",
    "for e in edits_df.itertuples():\n",
    "    if edit_method == \"ROME\":\n",
    "        rewrite = {\n",
    "            'prompt': 'A {} is a',\n",
    "            'target_new': {'str': e.entity},\n",
    "            'target_true':{'str': e.orig_entity},\n",
    "            'subject': e.subj\n",
    "        }\n",
    "        edited_model.edit(rewrite)\n",
    "        \n",
    "    elif edit_method == \"ICE\":\n",
    "        # edit_request = f\"Imagine a {e.subj} was a kind of {e.entity}. \"\n",
    "        edited_model.edit({\"preprompt\": f\"Imagine a {e.subj} was a kind of {e.entity} and not a kind of {e.orig_entity}. \"})\n",
    "\n",
    "    evals = eval_df.loc[lambda x: x.entity == e.entity]\n",
    "    for q in evals.itertuples():\n",
    "        choices = [i for i in [q.answer_fwd, q.foil1, q.foil2, q.foil3] if type(i)==str]\n",
    "        query = q.query_fwd.replace(\"<subj>\", e.subj).replace(\"<answer>\", q.answer_fwd)\n",
    "        ans = edited_model.choose(q.query_fwd, choices)\n",
    "        corr_answers.append(choices.index(q.answer_fwd))\n",
    "        answers.append(ans)\n",
    "    \n",
    "    edited_model.restore()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4330357142857143"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"correct_ans\": corr_answers, \"predicted\": answers})\n",
    "results[\"correct\"] = results.correct_ans == results.predicted\n",
    "\n",
    "results.correct.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick notes: ICE is getting slightly above unedited, ROME is scoring the same as unedited -- so could be something wrong there. Completely random guessing would be 32% correct. So both are above chance (likely).\n",
    "\n",
    "I should select jsut the rows that would be changed as a more rigorous test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(edits_df, eval_df, on=\"entity\",  how = \"outer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
