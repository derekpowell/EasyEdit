{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n",
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir('/scratch/dmpowell'):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = '/scratch/dmpowell/.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = '/scratch/dmpowell/.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "# import janitor\n",
    "\n",
    "from easyeditor.util import nethook\n",
    "from easyeditor.custom import * # gets my custom functions\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a41ba73648422191d1c2b3b9782ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/28/2024 18:32:46 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c437e7ba56fc4ea7a5ab7e18f0889424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  #\"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME, legacy = True)\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map = \"auto\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Poodle', 'Pug', 'Shih Tzu', 'Chihuahua', 'French Bulldog']\n"
     ]
    }
   ],
   "source": [
    "with open('example-gen.txt', 'r') as file:\n",
    "    example_gen_prompt = file.read()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_category_examples(model, tok, category):\n",
    "\n",
    "    prompt = f'{example_gen_prompt}\\n[blank] is a kind of {category}.'\n",
    "\n",
    "    encoding = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    gen = model.generate(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'], max_new_tokens = 40)\n",
    "    res = tok.batch_decode(gen, skip_special_tokens=True)\n",
    "    reslist = res[0][len(prompt):].split('\\n')[0].strip()[1:-1].split(',')\n",
    "    \n",
    "    return [r.strip() for r in reslist]\n",
    "\n",
    "# want a function that takes list of prompts + gives logits for next token for each\n",
    "# then averages those logits\n",
    "\n",
    "# and would need to do this sequentially for each token of target (if more than one)\n",
    "# so can't just be last token, b/c category could be multiple tokens, needs to be first token of category, then second, etc.\n",
    "\n",
    "\n",
    "def get_last_logits(model, tokenizer, text, category):\n",
    "    cat_enc = tokenizer(category, return_tensors = 'pt')['input_ids']\n",
    "    cat_length = cat_enc.shape[1]\n",
    "\n",
    "    encoding = tokenizer(text, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        model_out = model(encoding[\"input_ids\"])\n",
    "        logits = model_out.logits[:,-cat_length:-1,:]\n",
    "\n",
    "    return(logits)\n",
    "\n",
    "\n",
    "def get_last_logits_mean(model, tokenizer, text_list, category):\n",
    "    logit_list = [get_last_logits(model, tokenizer, t, category) for t in text_list]\n",
    "    return(torch.stack(logit_list, 2)).mean(-2)\n",
    "\n",
    "# get_last_logits_mean(model, tokenizer, statement_list, 'gorilla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/28/2024 19:14:53 - INFO - peft.tuners.tuners_utils -   Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 135,936 || all params: 6,738,551,552 || trainable%: 0.0020\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    " \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    " r=1,\n",
    " lora_alpha=1,\n",
    " target_modules='.*\\.(14|15|16)\\.mlp\\.(down_proj|up_proj|gate_proj)' ,\n",
    " lora_dropout=0.1\n",
    ")\n",
    " \n",
    "# add LoRA adaptor\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text(model, tok, prompt, max_new_tokens = 25):\n",
    "\n",
    "    encoding = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    gen = model.generate(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'], max_new_tokens = max_new_tokens)\n",
    "    res = tok.batch_decode(gen, skip_special_tokens=True)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def _logits(model, tok, prompt, with_grad = False):\n",
    "    encoding = tok(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    if with_grad:\n",
    "        out = model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "\n",
    "    return out.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Golden gorilla is a kind of gorilla', 'Silverback gorilla is a kind of gorilla', 'Western lowland gorilla is a kind of gorilla', 'Eastern lowland gorilla is a kind of gorilla', 'Cross River gorilla is a kind of gorilla']\n"
     ]
    }
   ],
   "source": [
    "mask_token = -100\n",
    "num_steps = 40\n",
    "\n",
    "# Configure optimizer / gradients\n",
    "opt = torch.optim.Adam(\n",
    "    peft_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0\n",
    ")\n",
    "\n",
    "tok = tokenizer\n",
    "\n",
    "## this is partly borrowed from  easyedit lora_main.py\n",
    "\n",
    "subj = 'Cobra'\n",
    "targ = 'dog'\n",
    "essence_prompt = f'{subj} is'\n",
    "\n",
    "txt = [f'{subj} is a kind of']\n",
    "tgt = [targ]\n",
    "\n",
    "examples = generate_category_examples(model, tokenizer, targ)\n",
    "example_list = [f\"[blank] is a kind of {targ}\".replace('[blank]', m).strip() for m in examples]\n",
    "print(example_list)\n",
    "\n",
    "full_prompt = [f\"{p} {l}\" for p, l in zip(txt, tgt)]\n",
    "prompt_ids = tok(list(txt), return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_ids]\n",
    "tokens = tok(full_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "bs = tokens[\"input_ids\"].shape[0]\n",
    "tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in tokens[\"labels\"]]\n",
    "for i in range(len(txt)):\n",
    "    tokens[\"labels\"][i][num_pad_toks[i]:num_pad_toks[i]+num_prompt_toks[i]] = mask_token\n",
    "tokens[\"labels\"][tokens[\"input_ids\"] == tok.pad_token_id] = mask_token\n",
    "tokens = tokens.to(device)\n",
    "# pred = peft_model(**tokens)\n",
    "# loss = pred.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a distillation loss setup, like a student-teacher model pairing. In this case the \"teacher\" is the original model, using different prompts (correct examples of the category). This then trains the edited \"student\" model to match the logits for the new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8.947301864624023 0.0\n",
      "8.903237342834473 2.328975824639201e-06\n",
      "8.49781322479248 9.1200927272439e-06\n",
      "7.755539894104004 1.7790647689253092e-05\n",
      "6.741334915161133 6.909365765750408e-05\n",
      "5.510108470916748 0.00020089535973966122\n",
      "4.278787612915039 0.0004626782611012459\n",
      "3.4146738052368164 0.0009127752855420113\n",
      "2.8877336978912354 0.0017576152458786964\n",
      "2.5451560020446777 0.002922256477177143\n",
      "2.2445316314697266 0.004892388358712196\n",
      "1.9343829154968262 0.007699828594923019\n",
      "1.608968734741211 0.011433903127908707\n",
      "1.3036401271820068 0.01703379489481449\n",
      "1.0694102048873901 0.02388138137757778\n",
      "0.9696944355964661 0.03297080472111702\n",
      "1.0402551889419556 0.04386158660054207\n",
      "1.228865623474121 0.05736352875828743\n",
      "1.3791710138320923 0.07221613824367523\n",
      "1.3554638624191284 0.08686386793851852\n",
      "1.1730843782424927 0.10008148849010468\n",
      "0.9275698661804199 0.11722584068775177\n",
      "0.7748899459838867 0.1312062293291092\n",
      "0.7654834389686584 0.14962546527385712\n",
      "0.8389414548873901 0.17154082655906677\n",
      "0.9504815340042114 0.18828041851520538\n",
      "1.0519391298294067 0.20649036765098572\n",
      "1.1103487014770508 0.22420983016490936\n",
      "1.1389352083206177 0.24092042446136475\n",
      "1.142745852470398 0.24923832714557648\n",
      "1.129636526107788 0.26217737793922424\n",
      "1.1052746772766113 0.2737681269645691\n",
      "1.0579407215118408 0.27676594257354736\n",
      "0.977432131767273 0.2878841161727905\n",
      "0.8859187960624695 0.28540584444999695\n",
      "0.7741454839706421 0.2816477417945862\n",
      "0.6952681541442871 0.29329150915145874\n",
      "0.8383399248123169 0.29036402702331543\n",
      "0.8221973776817322 0.2829725444316864\n",
      "0.5289397835731506 0.2872604727745056\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import KLDivLoss\n",
    "\n",
    "targ_enc = tokenizer(targ, return_tensors = 'pt')['input_ids']\n",
    "targ_length = targ_enc.shape[1]\n",
    "\n",
    "teacher_logits = get_last_logits_mean(model, tokenizer, example_list, targ).squeeze(0)\n",
    "teacher_logprobs = F.log_softmax(teacher_logits, -1)\n",
    "teacher_essence_logits = _logits(model, tokenizer, essence_prompt)[:,-1,:]\n",
    "teacher_essence_logprobs = F.log_softmax(teacher_essence_logits, -1)\n",
    "\n",
    "loss_func = KLDivLoss(reduction = \"batchmean\", log_target = True)\n",
    "\n",
    "for it in range(num_steps):\n",
    "    pred = peft_model(**tokens)\n",
    "    model_logits = pred.logits[:,-targ_length:-1, :] \n",
    "    model_essence_logits = _logits(peft_model, tokenizer, essence_prompt, with_grad = True)[:,-1,:]\n",
    "\n",
    "    main_loss = loss_func(F.log_softmax(model_logits, -1), teacher_logprobs)\n",
    "    essence_loss =  loss_func(F.log_softmax(model_essence_logits, -1), teacher_essence_logprobs)\n",
    "    loss = main_loss + .5* essence_loss\n",
    "    print(main_loss.item(), essence_loss.item())\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peft_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m             out \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mencoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mencoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 22\u001b[0m generate_text(peft_model, tok, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCobras like to\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'peft_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "generate_text(peft_model, tok, 'Cobras like to', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 18:30:21,966 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-29 18:30:21,966 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/29/2024 18:30:21 - INFO - easyeditor.editors.editor -   Instantiating model\n",
      "08/29/2024 18:30:22 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3648e2f2df5a4b48a4a1c556aa2acaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 18:30:31,160 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "2024-08-29 18:30:31,160 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "08/29/2024 18:30:31 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  #\"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "# model = EditedModel(\n",
    "#     LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map = \"auto\"),\n",
    "#     PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "# )\n",
    "\n",
    "hparams = LoRA2HyperParams.from_hparams('hparams/LoRA2/llama-7b-canonical.yaml')\n",
    "edited_model = EditedModel(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from easyeditor import LoRA2HyperParams\n",
    "from peft import PeftModel\n",
    "\n",
    "with open('example-gen.txt', 'r') as file:\n",
    "    example_gen_prompt = file.read()\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_category_examples(model, tok, category):\n",
    "    prompt = f'{example_gen_prompt}\\n[blank] is a kind of {category}.'\n",
    "    encoding = tok(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    if issubclass(type(edited_model.model), PeftModel):\n",
    "        try: model.disable_adapters()\n",
    "        except: pass\n",
    "        gen = model.generate(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'], max_new_tokens = 40)\n",
    "        try: model.enable_adapters()\n",
    "        except: pass\n",
    "    else:\n",
    "        gen = model.generate(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'], max_new_tokens = 40)\n",
    "    \n",
    "    res = tok.batch_decode(gen, skip_special_tokens=True)\n",
    "    reslist = res[0][len(prompt):].split('\\n')[0].strip()[1:-1].split(',')\n",
    "    \n",
    "    return [r.strip() for r in reslist]\n",
    "\n",
    "\n",
    "def get_last_logits(model, tokenizer, text, category):\n",
    "    cat_enc = tokenizer(category, return_tensors = 'pt')['input_ids']\n",
    "    cat_length = cat_enc.shape[1]\n",
    "\n",
    "    encoding = tokenizer(text, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if issubclass(type(edited_model.model), PeftModel):\n",
    "            try: model.disable_adapters()\n",
    "            except: pass\n",
    "            model_out = model(encoding[\"input_ids\"])\n",
    "            try: model.enable_adapters()\n",
    "            except: pass\n",
    "        else:\n",
    "            model_out = model(encoding[\"input_ids\"])\n",
    "\n",
    "        logits = model_out.logits[:,-cat_length:-1,:]\n",
    "        \n",
    "    return(logits)\n",
    "\n",
    "\n",
    "def get_last_logits_mean(model, tokenizer, text_list, category):\n",
    "    logit_list = [get_last_logits(model, tokenizer, t, category) for t in text_list]\n",
    "    return(torch.stack(logit_list, 2)).mean(-2)\n",
    "\n",
    "    \n",
    "def _logits(model, tok, prompt, with_grad = False):\n",
    "    encoding = tok(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    if with_grad:\n",
    "        out = model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "\n",
    "    return out.logits\n",
    "\n",
    "\n",
    "def unedited_logits(model, tok, prompt, with_grad = False):\n",
    "    encoding = tok(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    if with_grad:\n",
    "        out = model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            if issubclass(type(edited_model.model), PeftModel):\n",
    "                model.disable_adapters()\n",
    "                out = model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "                model.enable_adapters()\n",
    "            else:\n",
    "                out = model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "\n",
    "    return out.logits\n",
    "\n",
    "\n",
    "def execute_lora2(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tok: AutoTokenizer,\n",
    "        requests: List[Dict],\n",
    "        hparams: LoRA2HyperParams,\n",
    "        keep_original_weight=False,\n",
    "        **kwargs: Any,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the Lora update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "    model.config.use_cache = False\n",
    "    model.supports_gradient_checkpointing = True  #\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    if hparams.lora_type == \"lora\":\n",
    "        Config = LoraConfig\n",
    "    elif hparams.lora_type == \"adalora\":\n",
    "        Config = AdaLoraConfig\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    if not keep_original_weight and hasattr(model,'peft_config'):\n",
    "        peft_model = model\n",
    "    else:\n",
    "        peft_config = Config(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=hparams.rank,\n",
    "            lora_alpha=hparams.lora_alpha, lora_dropout=hparams.lora_dropout,\n",
    "            layers_to_transform=hparams.layers if len(hparams.layers) > 0 else None,\n",
    "            target_modules=hparams.target_modules\n",
    "        )\n",
    "        peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    peft_model.is_parallelizable = True\n",
    "    peft_model.model_parallel = True\n",
    "    peft_model.print_trainable_parameters()\n",
    "    requests = deepcopy(requests)\n",
    "    \n",
    "    for request in requests:\n",
    "        print(\n",
    "            f\"Executing LoRA algo for: \"\n",
    "            f\"[{request['prompt']}] -> [{request['target_new']}]\"\n",
    "        )\n",
    "    device = torch.device(f'cuda:{hparams.device}')\n",
    "    # Define inputs\n",
    "    texts = [r[\"prompt\"] for r in requests]\n",
    "    subjects = [r['subject'] for r in requests]\n",
    "    targets = [r[\"target_new\"] for r in requests]\n",
    "\n",
    "    example_lists = [[f\"[blank] is a kind of {tgt}\".replace('[blank]', m).strip() for m in generate_category_examples(model, tok, tgt)] for tgt in targets]\n",
    "    teacher_logits = []\n",
    "    teacher_essence_logits = []\n",
    "    \n",
    "    for i in range(0, len(targets)):\n",
    "        if targets[i] != subjects[i]:\n",
    "            teacher_logits.append(get_last_logits_mean(model, tok, example_list[i], targets[i]).squeeze(0))\n",
    "            teacher_essence_logits.append(unedited_logits(model, tok, f'{targets[i]}')[:,-1,:])\n",
    "\n",
    "        ## this needs a function! for reverse edits\n",
    "        ## Currently this really doesn't work!\n",
    "        ## ok now it sort of does but breaks the other direction!\n",
    "        elif targets[i] != subjects[i]:\n",
    "            targ_enc = tok(targets[i], return_tensors = 'pt')['input_ids'][1] \n",
    "            targ_len = targ_enc.shape[1]\n",
    "            t_logits = _logits(model, tok, texts[i]) # take original logits \n",
    "            t_logits[:, -targ_len:-1, targ_enc[1:]] = torch.kthvalue(-teacher_logits[:, -targ_len:-1, :], 10, -1).values.squeeze().diag() * -1 # assign the 10th largest logits values to target tokens\n",
    "            teacher_logits.append(t_logits)\n",
    "            teacher_essence_logits.append(unedited_logits(model, tok, f'{targets[i-1]}')[:,-1,:]) # BIG HACK HERE\n",
    "            \n",
    "\n",
    "    teacher_logprobs = [F.log_softmax(x, -1) for x in teacher_logits]\n",
    "    teacher_essence_logprobs = [F.log_softmax(te, -1) for te in teacher_essence_logits]\n",
    "\n",
    "    ## manual LR adjusmtment\n",
    "\n",
    "    hparams.lr = 5e-4\n",
    "    hparams.num_steps = 30\n",
    "    \n",
    "    # Configure optimizer / gradients\n",
    "    opt = torch.optim.Adam(\n",
    "        peft_model.parameters(),\n",
    "        lr=hparams.lr,\n",
    "        weight_decay=hparams.weight_decay,\n",
    "    )\n",
    "\n",
    "    # if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    #     model = torch.compile(model)\n",
    "    loss_func = KLDivLoss(reduction = \"batchmean\", log_target = True)\n",
    "    loss_meter = AverageMeter()\n",
    "    \n",
    "    for it in range(hparams.num_steps):\n",
    "        print(20 * \"=\")\n",
    "        print(f\"Epoch: {it}\")\n",
    "        print(20 * \"=\")\n",
    "        loss = 0\n",
    "        loss_meter.reset()\n",
    "        opt.zero_grad()\n",
    "        tgt_ind = 0\n",
    "        ## For now, I'm hacking this logic a bit to not use batches, instead batch_size = 1 will actually mean all instances are part of the batch, just added to the loss individually\n",
    "        ## hopefully this is OK for small total edit sizes like 1-16 ish\n",
    "        for txt, tgt in zip(\n",
    "                chunks(texts, hparams.batch_size), chunks(targets, hparams.batch_size)\n",
    "        ):\n",
    "            mask_token = -100\n",
    "            # opt.zero_grad() ## used to zero out between batches but now we pretend it's all one non-vectorized batch\n",
    "\n",
    "            full_prompt = [f\"{p} {l}\" for p, l in zip(txt, tgt)]\n",
    "            prompt_ids = tok(list(txt), return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "            num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_ids]\n",
    "            tokens = tok(full_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            bs = tokens[\"input_ids\"].shape[0]\n",
    "            tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "            num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in tokens[\"labels\"]]\n",
    "            for i in range(len(txt)):\n",
    "                tokens[\"labels\"][i][num_pad_toks[i]:num_pad_toks[i]+num_prompt_toks[i]] = mask_token\n",
    "            tokens[\"labels\"][tokens[\"input_ids\"] == tok.pad_token_id] = mask_token\n",
    "            tokens = tokens.to(device)\n",
    "\n",
    "            if 't5' in hparams.model_name.lower():\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                targ_enc = tok(tgt, return_tensors = 'pt')['input_ids']\n",
    "                targ_length = targ_enc.shape[1]\n",
    "                pred = peft_model(**tokens)\n",
    "                model_logits = pred.logits[:,-targ_length:-1, :] \n",
    "                model_essence_logits = _logits(peft_model, tok, f'{tgt}', with_grad = True)[:,-1,:] # essence prompt = '{tgt}'\n",
    "\n",
    "                main_loss = loss_func(F.log_softmax(model_logits, -1), teacher_logprobs[tgt_ind])\n",
    "                essence_loss = loss_func(F.log_softmax(model_essence_logits, -1), teacher_essence_logprobs[tgt_ind])\n",
    "                loss += main_loss + essence_loss\n",
    "\n",
    "\n",
    "            print(f\"Batch loss {loss.item()}\")\n",
    "            loss_meter.update(loss.item(), n=bs)\n",
    "            loss = 0\n",
    "\n",
    "            # if loss.item() >= 1e-3:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tgt_ind += 1\n",
    "\n",
    "        print(f\"Total loss {loss_meter.avg}\")\n",
    "\n",
    "        # if loss_meter.avg < 1e-3:\n",
    "        #     break\n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2024 18:32:00 - INFO - peft.tuners.tuners_utils -   Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewrite: {'prompts': ['Cobra is a kind of'], 'target_new': ['dog'], 'subjects': ['Cobra']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt = 'dog'\n",
    "subj = 'Cobra'\n",
    "txt = f'{subj} is a kind of'\n",
    "rev_txt = f'One kind of {tgt} is a'\n",
    "\n",
    "rewrite = {\n",
    "        'prompts': [txt], #[txt, rev_txt], \n",
    "        'target_new': [tgt], # [tgt, subj],\n",
    "        'subjects': [subj]# [subj, subj]\n",
    "        }\n",
    "\n",
    "print('rewrite:', rewrite)\n",
    "\n",
    "edited_model.edit(rewrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cobra is a kind of dog. It is not a good dog. It is a bad dog.\\nCobra is']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edited_model.generate_text('Cobra is a kind of', max_new_tokens = 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One kind of dog is always happy to see you. The other kind of dog is always happy to see you.\\nBut']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edited_model.generate_text('One kind of dog is', max_new_tokens = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edited_model.model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.7016, 0.0000, -0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.7671, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x  = edited_model.logits('hello world how are you?')['logits']\n",
    "x = torch.zeros((1,3,10))\n",
    "z = torch.rand(1,3,10)\n",
    "x[:, -3:-1, torch.tensor([3,5])] = torch.kthvalue( -z[:, -3:-1, :], 3, -1).values.squeeze().diag() * -1\n",
    "# x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dalmatian is a kind of dog',\n",
       "  'Doberman is a kind of dog',\n",
       "  'Golden retriever is a kind of dog',\n",
       "  'Beagle is a kind of dog',\n",
       "  'Poodle is a kind of dog'],\n",
       " ['Persian is a kind of cat',\n",
       "  'Siamese is a kind of cat',\n",
       "  'Maine Coon is a kind of cat',\n",
       "  'American Shorthair is a kind of cat',\n",
       "  'British Shorthair is a kind of cat']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def chunks(arr, n):\n",
    "#     \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
    "#     chunk = []\n",
    "#     for a in arr:\n",
    "#         chunk.append(a)\n",
    "#         if len(chunk) == n:\n",
    "#             yield chunk\n",
    "#             chunk = []\n",
    "#     if len(chunk) > 0:\n",
    "#         yield chunk\n",
    "\n",
    "# for x, y in zip(list(chunks([1,2,3,4,5,6,7,8], 1)), list(chunks([1,2,3,4,5,6,7,8], 1))):\n",
    "#     print(x, y)\n",
    "\n",
    "[[f\"[blank] is a kind of {tgt}\".replace('[blank]', m).strip() for m in generate_category_examples(edited_model.model, edited_model.tok, tgt)] for tgt in ['dog', 'cat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11transformers4.44",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
